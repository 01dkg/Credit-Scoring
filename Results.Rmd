---
title: "Results & Analysis"
output:
  html_document: default
  html_notebook: default
---
#Logistic Regression Model (Stepwise)
```{r}
lr <- glm(DefaultedLoans ~ InterestIncome + CreditRating+
    PropertyValue + LoanBalance + AnnualPYMT + LTV + 
    InterestType + NewLoan + ProbationaryLoans + MortgageYears + 
    MortgageType + InArrears + County + AddressLatitude + AddressLongitude, 
    family = "binomial", data = trainDatav2)
summary(lr)
```

```{r}

lr <- step(lr)

```
```{r}
summary(lr)
save(lr, file = "Model/lr.rda")
```

```{r}
significant.variables <- summary(lr)$coeff[-1,4] < 0.01
names(significant.variables)[significant.variables == TRUE]
```
```{r}
prob <- predict(lr, type = "response")
res <- residuals(lr, type = "deviance")

#Plot Residuals
plot(predict(lr), res,
     xlab="Fitted values", ylab = "Residuals",
     ylim = max(abs(res)) * c(-1,1))
```

```{r}
#score test data set
library(ROCR)
testDatav2$lr_score <- predict(lr,type='response',testDatav2)
lr_prediction <- prediction(testDatav2$lr_score, testDatav2$DefaultedLoans)
lr_performance <- performance(lr_prediction,"tpr","fpr")

#ROC
plot(lr_performance, lwd=2, colorize=TRUE, main="ROC m1: Logistic Regression Performance")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)
```
```{r}
# Plot precision/recall curve
lr_performance_precision <- performance(lr_prediction, measure = "prec", x.measure = "rec")
plot(lr_performance_precision, main="LR Logistic:Precision vs Recall curve")
```

```{r}
# Plot accuracy as function of threshold
lr_performance_accuracy <- performance(lr_prediction, measure = "acc")
plot(lr_performance_accuracy, main="LR Logistic:Accuracy as function of threshold")
```

```{r}
#KS, Gini & AUC m1
lr_KS <- round(max(attr(lr_performance,'y.values')[[1]]-attr(lr_performance,'x.values')[[1]])*100, 2)
lr_AUROC <- round(performance(lr_prediction, measure = "auc")@y.values[[1]]*100, 2)
lr_Gini <- (2*lr_AUROC - 100)
cat("AUROC: ",lr_AUROC,"\tKS: ", lr_KS, "\tGini:", lr_Gini, "\n")
```
```{r}
confusion.glm <- function(data, model) {
  prediction <- ifelse(predict(model, data, type='response') > 0.5, TRUE, FALSE)
  confusion  <- table(prediction, as.logical(model$y))
  confusion  <- cbind(confusion, c(1 - confusion[1,1]/(confusion[1,1]+confusion[2,1]), 1 - confusion[2,2]/(confusion[2,2]+confusion[1,2])))
  confusion  <- as.data.frame(confusion)
  names(confusion) <- c('FALSE', 'TRUE', 'class.error')
  confusion
}

confusion.glm(testDatav2$lr_prediction,lr)
```

# Decision Tree Results
```{r}
#Decision Tree for Tableu
library(rpart)
library(rattle)					# Fancy tree plot
library(rpart.plot)			# Enhanced tree plots
library(RColorBrewer)		# Color selection for fancy tree plot
library(party)					# Alternative decision tree algorithm
library(partykit)				# Convert rpart object to BinaryTree
library(caret)
library(tree)
DT <- rpart(DefaultedLoans ~ InterestIncome + CreditRating+
    PropertyValue + LoanBalance + AnnualPYMT + LTV + 
    InterestType + NewLoan + ProbationaryLoans + MortgageYears + 
    MortgageType + InArrears + County + AddressLatitude + AddressLongitude
    ,method = "class",data=trainDatav2,
            control =  rpart.control(minisplit=5,cp = 0.001))

save(fit, file = "Model/DT.rda")
print(DT)
prp(DT)
tree.1 <- DT
fancyRpartPlot(tree.1)
```
```{r}
# score test data
testDatav2$DT_score <- predict(DT,type='prob',testDatav2)
DT_prediction <- prediction(testDatav2$DT_score[,2],testDatav2$DefaultedLoans)
DT_performance <- performance(DT_prediction,"tpr","fpr")

# MOdel performance plot
plot(DT_performance, lwd=2, colorize=TRUE, main="ROC Decision Tree: Traditional Recursive Partitioning")
lines(x=c(0, 1), y=c(0, 1), col="red", lwd=1, lty=3);
lines(x=c(1, 0), y=c(0, 1), col="green", lwd=1, lty=4)
```
```{r}
# Plot precision/recall curve
DT_performance_precision <- performance(DT_prediction, measure = "prec", x.measure = "rec")
plot(DT_performance_precision, main="Decision Tree vs Precision/recall curve")
```
```{r}
# Plot accuracy as function of threshold
DT_perf_acc <- performance(DT_prediction, measure = "acc")
plot(DT_perf_acc, main="m2 Recursive Partitioning:Accuracy as function of threshold")
```

```{r}
# KS & AUC DT
DT_AUROC <- round(performance(DT_prediction, measure = "auc")@y.values[[1]]*100, 2)
DT_KS <- round(max(attr(DT_performance,'y.values')[[1]]-attr(DT_performance,'x.values')[[1]])*100, 2)
DT_Gini <- (2*DT_AUROC - 100)
cat("AUROC: ",DT_AUROC,"\tKS: ", DT_KS, "\tGini:", DT_Gini, "\n")
```
```{r}
defaultPredict <- predict(DT, trainDatav2, type = 'class')
matrix <- table(defaultPredict, trainDatav2$DefaultedLoans)
print(matrix)
print((matrix[4]+matrix[1])/nrow(trainDatav2))
```

```{r}
plot(lr_performance, col='blue', lty=1, main='ROCs: Model Performance Comparision') # logistic regression
plot(DT_performance, col='red',lty=2, add=TRUE); # simple 
legend(0.6,0.5,
           c('LR :Logistic regression','DT: Decision tree'),
           col=c('blue','red'),
           lwd=3);
lines(c(0,1),c(0,1),col = "gray", lty = 4 ) # random line
```


